{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qrTiVkzjz21"
   },
   "source": [
    "# Tutorial on Machine Learning: Overview and Workflow\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9HqxmHz3Yin"
   },
   "source": [
    "## Want to use Colab? [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/CU-Denver-MathStats-OER/CDHE-OER-Presentations/blob/master/Tutorials/Basics_of_ML_in_Python_Tutorial.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zr8tSdzvEAg"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "***Disclaimer:*** \n",
    "\n",
    "- These tutorials are meant to convey high-level concepts and also show some advanced programming techniques and tricks. While some details are included for the sake of completeness or provide additional references for the interested reader, they are generally not required in order to follow the logical progression of ideas. \n",
    "\n",
    "- Moreover, you are not expected to know or understand all the details in great depth. In many cases, we specifically link details to specific courses so that you can plan your schedule around what you are most interested in learning.  \n",
    "\n",
    "***Level of assumed pre-requisite knowledge:*** \n",
    "\n",
    "- A \"pre-calculus\" level of mathematical proficiency is assumed that covers some [geometry formulas for areas/volumes of common shapes](https://en.wikipedia.org/wiki/List_of_formulas_in_elementary_geometry), [college algebra](https://en.wikipedia.org/wiki/Elementary_algebra), and also some familiarity with [common trigonometric functions](https://en.wikipedia.org/wiki/Trigonometric_functions). \n",
    "\n",
    "- Basic statistics revolving around the concept of a [line of best fit](https://en.wikipedia.org/wiki/Simple_linear_regression), i.e., fitting a line through data.\n",
    "\n",
    "- Introductory-level programming knowledge (in any language) involving variable types, user-defined functions, if-else statements, and for-loops. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBgIWciIxcg2"
   },
   "source": [
    "## ML, statistics, and optimization: The Big Picture\n",
    "---\n",
    "\n",
    "> Before machine learning, there was statistics...\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/828/1*x7P7gqjo8k2_bj2rTQWAfg.webp\" width=50%>\n",
    "\n",
    "- We briefly review [linear regression](https://en.wikipedia.org/wiki/Linear_regression) to help set the stage and mindset for learning models from data. \n",
    "\n",
    "    - Linear regression is a commonly studied topic in statistics and computational science. \n",
    "\n",
    "    - Linear regression concepts are also at the core of many ML algorithms, which makes it a great place to start our studies.\n",
    "\n",
    "    - Moreover, the standard formulation of linear regression problems share many conceptual and quantitative features with ML algorithms in terms of \"learning\" some model parameters that help minimize some cost functional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaG7B6JIGfJQ"
   },
   "source": [
    "\n",
    "> At the foundation of both regression and ML is *optimization.* But, underneath that is the bedrock, which is made of *linear algebra* (e.g., MATH 3191: Applied Linear Algebra).\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://imgs.xkcd.com/comics/machine_learning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFZR9fIOKj3u"
   },
   "source": [
    "## Want to study deeper?\n",
    "---\n",
    "\n",
    "- To go much more in-depth on linear regression, see Dr. Joshua French's OER textbook [A Progressive Introduction to Linear Models](https://cu-denver-mathstats-oer.github.io/Applied-Regression-Analysis/), which is developed for the MATH 4387/5387: Applied Regression Analysis course at CU Denver.\n",
    "\n",
    "- Alternatively, watch some [videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/) or read this [quick reference guide](http://www.dataschool.io/applying-and-interpreting-linear-regression/) to various key points.\n",
    "\n",
    "- Want a gentler introduction that served as the main motivation for this notebook? Check out the [prologue lecture](https://github.com/CU-Denver-MathStats-OER/Programming-for-Data-Science/blob/main/Lectures-and-Assignments/06-Machine-Learning/lectures/06_lecture_prologue.ipynb) for module 06 in the MATH 1376: Programming for Data Science course for more details on regression and the [part a lecture](https://github.com/CU-Denver-MathStats-OER/Programming-for-Data-Science/blob/main/Lectures-and-Assignments/06-Machine-Learning/lectures/06_lecture_part_a.ipynb) for the same module that goes into deeper details on classifiers for ML.\n",
    "\n",
    "- More advanced? Check out [scikit-learn](https://scikit-learn.org/stable/) examples that are available as Jupyter notebooks (but lack quite a bit of text). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNV3Oir2IUUv"
   },
   "source": [
    "## Libraries\n",
    "---\n",
    "\n",
    "- [scikit-learn](http://scikit-learn.org/stable/) is a popular module that you can use to perform linear regression and machine learning. \n",
    "\n",
    "    - You could devote an entire course to this library (and still only scratch the surface of what it has to offer).\n",
    "\n",
    "    - It is a good module for you to focus most of your energy/attention as you learn machine learning/data science since it provides a lot of the functionality for machine learning that you would in general need to use.\n",
    "\n",
    "- [Pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html) is a very impressive module that you may become intimately familiar with if you ever do any serious data analysis. The name is derived from the econometrics term [*panel data*](https://en.wikipedia.org/wiki/Panel_data#:~:text=In%20statistics%20and%20econometrics%2C%20panel,the%20same%20firms%20or%20individuals.) (sometimes referred to as longitudinal data by statisticians) that describes multi-dimensional data involving measurements over time. From the overview of pandas:\n",
    "\n",
    "    > ...[provides] fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. [Pandas] aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis / manipulation tool available in any language. It is already well on its way toward this goal.\n",
    "\n",
    "    - I highly recommend taking 3-5 minutes and just reading the full [overview of pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html).\n",
    "   \n",
    "-   Pandas has its own method for scraping specific types of data from HTML. \n",
    "    - Specifically, the method [`read_html`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html) will read HTML tables into a *list* of DataFrame objects whereas other `read_` commands exist for other more specialized purposes like [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) for reading csv files directly into a DataFrame.\n",
    "\n",
    "    - There is a rather thorough [10 minute introductory tutorial](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) to pandas that I recommend going through. \n",
    "\n",
    "        - In fact, you can easily take that 10 minute tutorial and turn it into your own Jupyter notebook, which is a good activity that I highly recommend.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CbErGggxcg4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSwYnzNbxcg9"
   },
   "source": [
    "Let's take a look at some data, ask some questions about that data, and then use linear regression to answer those questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjOilRxMxcg-"
   },
   "outputs": [],
   "source": [
    "# read data into a DataFrame\n",
    "url = 'https://github.com/CU-Denver-MathStats-OER/Programming-for-Data-Science/blob/main/Lectures-and-Assignments/06-Machine-Learning/lectures/Advertising.csv?raw=True'\n",
    "data = pd.read_csv(url, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBQ9X5iJxchA"
   },
   "outputs": [],
   "source": [
    "# print the shape of the DataFrame\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSp43b5Yxcg-"
   },
   "outputs": [],
   "source": [
    "data  # To get a \"big picture\" view of all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_NZv0sNxcg_"
   },
   "outputs": [],
   "source": [
    "data.head()  # just the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ez8wQuWaxcg_"
   },
   "source": [
    "### Some terminology\n",
    "---\n",
    "\n",
    "What are the **features**?\n",
    "\n",
    "- TV: advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n",
    "\n",
    "- Radio: advertising dollars spent on Radio\n",
    "\n",
    "- Newspaper: advertising dollars spent on Newspaper\n",
    "\n",
    "What is the **response**?\n",
    "\n",
    "- Sales: sales of a single product in a given market (in thousands of widgets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHjFaQF3xchA"
   },
   "source": [
    "There are 200 **observations**, and thus 200 **markets** in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYFR4JodxchA"
   },
   "outputs": [],
   "source": [
    "# visualize the relationship between the features and the response using scatterplots\n",
    "fig, axs = plt.subplots(1, 3, sharey=True)\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "data.plot(kind='scatter', x='TV', y='Sales', ax=axs[0], figsize=(16, 8))\n",
    "data.plot(kind='scatter', x='Radio', y='Sales', ax=axs[1])\n",
    "data.plot(kind='scatter', x='Newspaper', y='Sales', ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgbhZyz8xchB"
   },
   "source": [
    "## Questions About the Advertising Data\n",
    "---\n",
    "\n",
    "Let's pretend you work for the company that manufactures and markets this widget. The company might ask you the following: On the basis of this data, how should we spend our advertising money in the future to maximize profit?\n",
    "\n",
    "This general question might lead you to more specific questions:\n",
    "\n",
    "1. Is there a relationship between ads and sales?\n",
    "\n",
    "2. How strong is that relationship?\n",
    "\n",
    "3. Which ad types contribute the most to sales?\n",
    "\n",
    "4. What is the effect of each ad type of sales?\n",
    "\n",
    "5. Given ad spending in a particular market, can sales be predicted?\n",
    "\n",
    "Before we explore those, we may want to consider which scatter plot of data above even suggests there exists a clear functional relationship between a type of advertising expense and sales. The following cartoon contains a lot of wisdom.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://www.explainxkcd.com/wiki/images/9/91/linear_regression.png\">\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiXRgewrxchB"
   },
   "source": [
    "## Simple Linear Regression\n",
    "---\n",
    "\n",
    "Simple linear regression is an approach for predicting a **quantitative response** using a **single feature** (or \"predictor\" or \"input variable\"). It takes the following form:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x + \\epsilon$\n",
    "\n",
    "What does each term represent?\n",
    "\n",
    "- $y$ is the response\n",
    "- $x$ is the feature\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for x\n",
    "- $\\epsilon$ is deviation of value from the average behavior\n",
    "\n",
    "Together, $\\beta_0$ and $\\beta_1$ are called the **model coefficients**. To create your model, you must \"learn\" the values of these coefficients. And once we've learned these coefficients, we can use the model to predict sales!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9zjepp0xchB"
   },
   "source": [
    "## Estimating (\"Learning\") Model Coefficients\n",
    "---\n",
    "\n",
    "Generally speaking, coefficients are estimated using the **least squares criterion**, which means we find the line (mathematically) that minimizes the **sum of squared residuals** (or \"sum of squared errors\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPJjIU0jxchC"
   },
   "source": [
    "<img src=\"https://github.com/CU-Denver-MathStats-OER/Programming-for-Data-Science/blob/main/Lectures-and-Assignments/06-Machine-Learning/lectures/06_estimating_coefficients.png?raw=1\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMvPCO5TxchC"
   },
   "source": [
    "Let's break down the graphic.\n",
    "\n",
    "- The black dots are the **observed values** of x and y.\n",
    "\n",
    "- The blue line is our **least squares line**.\n",
    "\n",
    "- The red lines are the **residuals**, which are the distances between the observed values and the least squares line.\n",
    "\n",
    "How do the model coefficients relate to the least squares line?\n",
    "\n",
    "- $\\beta_0$ is the **intercept** (the typical value of $y$ when $x$=0)\n",
    "\n",
    "- $\\beta_1$ is the **slope** (the typical change in $y$ divided by change in $x$)\n",
    "\n",
    "Here is a graphical depiction of those calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LW4rVK7qxchC"
   },
   "source": [
    "<img src=\"https://github.com/CU-Denver-MathStats-OER/Programming-for-Data-Science/blob/main/Lectures-and-Assignments/06-Machine-Learning/lectures/06_slope_intercept.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xId_HntS4Aah"
   },
   "source": [
    "Thus, in the $SS_\\text{residuals}$ formula, the $\\hat{y}_i$ predictions are obtained by evaluating the model (i.e., the function defining the line) at the known $x_i$. In other words, \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\large \\hat{y}_i = \\beta_0+\\beta_1x_i, \n",
    "$$\n",
    "\n",
    "which implies\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\large    SS_\\text{residuals} = \\sum_{i=1}^N \\left(\\beta_0 + \\beta_1x_i - y_i\\right)^2.\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "To ***learn*** the model coefficients simply means to solve the following optimization problem\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "    \\large \\text{minimize}\\ SS_\\text{residuals}= \\sum_{i=1}^N \\left(\\beta_0 + \\beta_1x_i - y_i\\right)^2.\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "This is very similar to the problem of optimizing the jar shapes in the previous tutorial except now there are no constraints on $\\beta_0$ and $\\beta_1$. While there are two parameters in this problem, the \"cost functional\" ($SS_\\text{residuals}$) is a quadratic function of $\\beta_0$ and $\\beta_1$, which means that if we visualize $SS_\\text{residuals}$ as a function over the $(\\beta_0,\\beta_1)$ plane, we \"see\" something like shown in the code below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21cDnqx953dD"
   },
   "outputs": [],
   "source": [
    "def compute_SS_residuals(beta_0, beta_1, x, y):\n",
    "    return np.sum(np.power(beta_0 + beta_1*x - y, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kayagj9D7mqd"
   },
   "outputs": [],
   "source": [
    "# For illustrative purposes only\n",
    "nplot = 25\n",
    "true_beta_0 = 1\n",
    "true_beta_1 = 2\n",
    "\n",
    "r = np.linspace(0, 0.25, nplot)\n",
    "theta = np.linspace(0, 2*np.pi, nplot)\n",
    "r, theta = np.meshgrid(r, theta)\n",
    "beta_0s = true_beta_0 + r*np.cos(theta)/2\n",
    "beta_1s = true_beta_1 + r*np.sin(theta)\n",
    "\n",
    "num_rand_pts = 10\n",
    "x = -1 + 2*np.random.rand(num_rand_pts)\n",
    "y = true_beta_0 + true_beta_1*x\n",
    "\n",
    "SS_residuals = np.zeros((nplot, nplot))\n",
    "for i in range(nplot):\n",
    "    for j in range(nplot):\n",
    "        SS_residuals[j, i] = compute_SS_residuals(beta_0s[j, i], beta_1s[j, i], \n",
    "                                                  x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQrKmtXv72is"
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,7), subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "surf = ax.plot_surface(beta_0s, beta_1s, SS_residuals, cmap=cm.coolwarm,\n",
    "                       linewidth=0, alpha=0.9)\n",
    "\n",
    "ax.view_init(elev=25)\n",
    "ax.set_xlabel(r'$\\beta_0$'), ax.set_ylabel(r'$\\beta_1$')\n",
    "ax.set_title('The $SS_{residual}$ cost functional \"bowl\"')\n",
    "ax.set_xticks([]), ax.set_yticks([]), ax.set_zticks([])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrBarC2YxchD"
   },
   "source": [
    "- Clearly, there is a unique minimum to the \"bowl\" shape we see above.  \n",
    "\n",
    "- The ***learning*** of $\\beta_0$ and $\\beta_1$ that is done is simply trying to estimate the values that position the $SS_\\text{residual}$ value at the bottom of this bowl.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3td381EIjg_"
   },
   "source": [
    "### A common workflow\n",
    "---\n",
    "\n",
    "We now estimate (i.e., learn) the model coefficients for sales as a function of TV advertising data using linear regression. \n",
    "\n",
    "Check out the code documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "The 3 steps we use below are so often repeated in ML libraries that they are worth remembering:\n",
    "\n",
    "(1) ***import*** the model class from the ML library\n",
    "\n",
    "(2) ***instantiate*** a model object based on this imported class\n",
    "\n",
    "(3) ***fit*** (meaning learn) the parameters of the model using the available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xk-NWaGRxchD"
   },
   "outputs": [],
   "source": [
    "# create feature vector x and response vector y\n",
    "feature_cols = ['TV']\n",
    "x = data[feature_cols]\n",
    "y = data.Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDrDMJU1xchD"
   },
   "outputs": [],
   "source": [
    "# follow the usual sklearn pattern: (1) import, (2) instantiate, (3) fit\n",
    "\n",
    "# First we import\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Then, we instantiate a \"linear regression\" object\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Finally, we call the fit method in the object to \"learn\" the linear relationship\n",
    "lm.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3pCCh8I2tFT"
   },
   "outputs": [],
   "source": [
    "# print intercept and coefficients that were \"learned\" by the fit method and stored\n",
    "# as data attributes within the object\n",
    "print(lm.intercept_) \n",
    "print(lm.coef_)  # Shows up as an array because there are usually many features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kI6VLDJxchD"
   },
   "source": [
    "## Interpreting Model Coefficients\n",
    "---\n",
    "\n",
    "How do we interpret the TV coefficient ($\\beta_1$)?\n",
    "\n",
    "- A \"unit\" increase in TV ad spending is **associated with** a 0.047537 \"unit\" increase in Sales.\n",
    "\n",
    "- Or more clearly: An additional $1,000 spent on TV ads is **associated with** an increase in sales of 47.537 widgets.\n",
    "\n",
    "Note that if an increase in TV ad spending was associated with a **decrease** in sales, $\\beta_1$ would be **negative**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0Tnw5fGxchE"
   },
   "source": [
    "## Using the Model for Prediction\n",
    "---\n",
    "\n",
    "Let's say that there was a new market where the TV advertising spend was **$50,000**. What would we predict for the Sales in that market?\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1x$$\n",
    "$$\\hat{y} = 7.032594 + 0.047537 \\times 50$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xc9LW5Z6xchE"
   },
   "outputs": [],
   "source": [
    "# manually calculate the prediction\n",
    "7.032594 + 0.047537*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "edOsLDnV3ogQ"
   },
   "outputs": [],
   "source": [
    "# manually calculate the prediction in a slightly better way\n",
    "lm.intercept_ + lm.coef_[0]*50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMlpuB2DxchE"
   },
   "source": [
    "Thus, we would predict Sales of **9,409 widgets** in that market.\n",
    "\n",
    "Of course, we could (and *should*) also use the linear model object to make the prediction! \n",
    "\n",
    "***In general, it is expecting that there are multiple features in the model (as we later see), so we need to formulate the input as either a 2-D array or as a DataFrame.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would delete the array example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrC-GzA5xchF"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqTH5_-SxchF"
   },
   "outputs": [],
   "source": [
    "x_predict_array = np.array([[50]])  # Notice the double brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlK4xMWixchF"
   },
   "outputs": [],
   "source": [
    "lm.predict(x_predict_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rijx2MBLxchF"
   },
   "outputs": [],
   "source": [
    "# This is probably the better way to do it since it aligns\n",
    "# with the whole idea of using DataFrames to begin with.\n",
    "x_predict_df = pd.DataFrame({'TV': [50]})\n",
    "x_predict_df.head() # look at the \"head\" of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpEsQ_39xchG"
   },
   "outputs": [],
   "source": [
    "# use the model to make predictions on a new value\n",
    "lm.predict(x_predict_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1T1nHfgxchG"
   },
   "source": [
    "## Plotting the Least Squares Line\n",
    "---\n",
    "\n",
    "Let's make predictions for the **smallest and largest observed values of x**, and then use the predicted values to plot the least squares line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RL21gf2wxchG"
   },
   "outputs": [],
   "source": [
    "# create a DataFrame with the minimum and maximum values of TV\n",
    "x_new = pd.DataFrame({'TV': [data.TV.min(), data.TV.max()]})\n",
    "x_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1BQvvC5xchH"
   },
   "outputs": [],
   "source": [
    "# make predictions for those x values and store them\n",
    "linear_preds = lm.predict(x_new)\n",
    "linear_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xi3o8UTixchH"
   },
   "outputs": [],
   "source": [
    "# first, plot the observed data\n",
    "data.plot(kind='scatter', x='TV', y='Sales')\n",
    "\n",
    "# then, plot the least squares line\n",
    "plt.plot(x_new, linear_preds, c='red', linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1H3xrolxchH"
   },
   "source": [
    "## How Well Does the Model Fit the data?\n",
    "---\n",
    "\n",
    "The most common way to evaluate the overall fit of a linear model is by the **R-squared** value. \n",
    "\n",
    "The R-squared value:\n",
    "\n",
    "- Measures the **proportion of variance explained**, meaning the proportion of variance in the observed data that is explained by the model\n",
    "- Alternatively, measures the proportional reduction in $SS_{residuals}$ compared to the the **null model** that only has an intercept and no slope.\n",
    "- Is between 0 and 1, and generally, higher is better because it means that more variance is explained by the model.\n",
    "\n",
    "Here's an example of what R-squared \"looks like\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdVWdw-lxchI"
   },
   "source": [
    "<img src=\"https://github.com/CU-Denver-MathStats-OER/Programming-for-Data-Science/blob/main/Lectures-and-Assignments/06-Machine-Learning/lectures/06_r_squared.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjHqTRRrxchI"
   },
   "source": [
    "You can see that the **blue line** explains some of the variance in the data (R-squared=0.54), the **green line** explains more of the variance (R-squared=0.64), and the **red line** fits the training data even further (R-squared=0.66).\n",
    "\n",
    "- Does the red line look overly complicated for the data we want to fit?\n",
    "\n",
    "\n",
    "Let's calculate the R-squared value for our simple linear model using the score method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLJKx1B3xchI"
   },
   "outputs": [],
   "source": [
    "# print the R-squared value for the model\n",
    "lm.score(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLrIdv4BxchI"
   },
   "source": [
    "Is that a \"good\" R-squared value? It's hard to say. The threshold for a good R-squared value depends widely on the domain. Therefore, it's most useful as a tool for **comparing different models**. This is a topic best studied in more depth in a course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZGNSjilxchM"
   },
   "source": [
    "## Multiple Linear Regression\n",
    "---\n",
    "\n",
    "Simple linear regression can easily be extended to include multiple features. This is called **multiple linear regression**:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n + \\epsilon$\n",
    "\n",
    "Each $x$ represents a different feature, and each feature has its own coefficient. In this case:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times Radio + \\beta_3 \\times Newspaper + \\epsilon$\n",
    "\n",
    "We estimate these coefficients below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoVVhG-lxchM"
   },
   "outputs": [],
   "source": [
    "# create X and y\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper']\n",
    "X = data[feature_cols]\n",
    "y = data.Sales\n",
    "\n",
    "lm_multiple = LinearRegression()  # Instantiate (no need to import again)\n",
    "lm_multiple.fit(X, y)  # Fit\n",
    "\n",
    "# print intercept and coefficients\n",
    "print(lm_multiple.intercept_)\n",
    "print(lm_multiple.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6CYxYz3xchM"
   },
   "source": [
    "How do we interpret these coefficients?\n",
    "\n",
    "- For a given amount of Radio and Newspaper ad spending, an **increase of $1000 in TV ad spending** is associated with an **increase in Sales of 45.765 widgets**.\n",
    "- This interprets the first coefficient. How would you interpret the other coefficients?\n",
    "\n",
    "How well does our model fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the R-squared value for our the multiple linear regression model\n",
    "lm_multiple.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbiqZmdFxchN"
   },
   "source": [
    "## Feature Selection\n",
    "---\n",
    "\n",
    "How should we decide **which features to include** in a linear model to balance simplicity with a goodness of fit?\n",
    "\n",
    "**R-squared will always increase as you add more features to the model**, so it is not a reliable approach for choosing the best linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1o8BCAexchN"
   },
   "source": [
    "In practice, feature selection is often performed by combining a measure of model fit with something called [**Cross-validation**](https://en.wikipedia.org/wiki/Cross-validation_(statistics)), which provides a more reliable estimate of prediction error.\n",
    "\n",
    "- Cross-validation is a better way to choose which of your models will best **generalize** to out-of-sample data. \n",
    "\n",
    "- This is a topic that is best investigated in depth in a more advanced course (e.g., MATH 4337/5337: Intro to Statistical and Machine Learning, MATH 4388/5388: Machine Learning Methods, or MATH 6388: Statistical and Machine Learning).\n",
    "\n",
    "There is extensive functionality for cross-validation in scikit-learn, including automated methods for searching different sets of parameters and different models. Cross-validation can be applied to any model,  not only linear models, so we can use it for machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaJi6scz9JwF"
   },
   "source": [
    "## Classifiers (artificial neurons): Concept and Connection to Linear Regression\n",
    "---\n",
    "\n",
    "The perceptron algorithm is a very simple version of a neural network or deep learning model, so it is a great place to start when entering the field of machine leaning.\n",
    "\n",
    "From Wikipedia (emphasis my own):\n",
    "\n",
    "> In machine learning, the perceptron is an algorithm for **supervised learning** of **binary classifiers**. A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.\n",
    "\n",
    "What does **supervised learning** mean? Simply put, it means we learn a function that maps inputs to outputs based on *training data* containing example input-output pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRBabK2l9JwF"
   },
   "source": [
    "### The mathematics (not as bad as it looks and we are going to skip most of this)\n",
    "---\n",
    "\n",
    "Let $\\mathbf{x}=(x_1,x_2,\\ldots,x_m)$ denote the $m$ feature values present in the data and $\\mathbf{w}=(w_1,w_2,\\ldots, w_m)$ denote a collection of $m$ weights. The inner product of these two objects is $$\\mathbf{w}\\cdot\\mathbf{x}=\\sum_{i=1}^m w_ix_i,$$ i.e., it is the sum of the product of each pair $(x_i, w_i)$ over all observations.\n",
    "\n",
    "\n",
    " A linear binary classifier defined on $\\mathbf{x}$ is represented as the function \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\large    f(\\mathbf{w}\\cdot \\mathbf{x} + b) = f\\biggl(\\sum_{i=1}^m w_i x_i + b\\biggr) = \\begin{cases}\n",
    "                        1, & \\sum_{i=1}^m w_i x_i + b>0, \\\\\n",
    "                        -1, & \\text{else}.\n",
    "                    \\end{cases}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Here, $b$ denotes something called the bias. \n",
    "\n",
    "We can compute $\\mathbf{w}\\cdot\\mathbf{x}=\\sum_{i=1}^m w_ix_i$ using the `numpy.dot` function, which takes two arrays as arguments.\n",
    "\n",
    "\n",
    "We use the perceptron to make binary decisions, i.e., whether or not data that possess $m$ identifying features should belong to one set or its complement (which are represented by the outputs 1 and -1, respectively). \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/CU-Denver-MathStats-OER/Programming-for-Data-Science/blob/main/Lectures-and-Assignments/06-Machine-Learning/lectures/neuron_and_perceptron.png?raw=1\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does a perceptron optimize?\n",
    "---\n",
    "\n",
    "A perceptron tries to minimizes our classification error. \n",
    "\n",
    "Ideally, we want our data set to be ***linearly separable***. If our data set is linearly separable, then we can create a perfect classifier.\n",
    "\n",
    "What does linearly separable mean? It means that we can construct a line (for 2-dimensional input data sets), a plane (for 3-dimensional input data sets), or hyperplanes (for 4- and higher-dimensional input data sets) that *separates* the input data space into two parts associated with the output classes (i.e., the values of $y$). \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/CU-Denver-MathStats-OER/Programming-for-Data-Science/blob/main/Lectures-and-Assignments/06-Machine-Learning/lectures/linearly_separable_example.png?raw=1\" title='Linearly separable example' width=40%>\n",
    "\n",
    "<br>\n",
    "\n",
    "See the following for some more background and illustrative examples:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Linear_separability\n",
    "\n",
    "- https://www.commonlounge.com/discussion/6caf49570d9c4d0789afbc544b32cdbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfADtq7_9JwG"
   },
   "source": [
    "### Comparing regression and a perceptron\n",
    "---\n",
    "\n",
    "Similarities:\n",
    "\n",
    "- Regression and a perceptron try to find the line that \"best fits\" the data, i.e. the line that minimizes our prediction or classification error.\n",
    "\n",
    "Differences:\n",
    "\n",
    "- The output of the regression function (the line of best fit) can be any number between $-\\infty$ and $\\infty$, depending on the input.\n",
    "- The output of the perceptron can only be -1 or 1, depending on the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NrCaerl9JwL"
   },
   "source": [
    "### Real data isn't linearly separable\n",
    "---\n",
    "\n",
    "John Hollinger is a former ESPN analyst/writer who developed some advanced statistics to quantify efficiencies of players and teams in the NBA. He created PER (Player Efficiency Rating), which is often used to evaluate how effective a player is. His statistics were so useful that he parlayed his creation into becoming the VP of basketball operations for the Memphis Grizzlies in 2012.\n",
    "\n",
    "Below, we *crawl* through some online data courtesy of ESPN. By courtesy of ESPN what I actually mean is that according to the terms of service from Disney (the parent company of ESPN): \n",
    "\n",
    "> G. Informational and Entertainment Purposes. You understand that the Disney Products are for your personal, noncommercial use and are intended for informational and entertainment purposes only; the content available does not constitute legal, financial, professional, medical or healthcare advice or diagnosis and cannot be used for such purposes.\n",
    "\n",
    "Anyway, let's see how well team efficiency statistics in the regular season correspond to a predictor of *success* defined by a team making the playoffs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ls-Snnma9JwL"
   },
   "outputs": [],
   "source": [
    "# Let's first orient ourselves on the types of data sets we will be using\n",
    "df_regular_season_temp = pd.read_html('http://www.espn.com/nba/hollinger/teamstats/_/year/2019')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmmDmHj79JwL"
   },
   "outputs": [],
   "source": [
    "df_postseason_temp = pd.read_html('http://www.espn.com/nba/hollinger/teamstats/_/year/2019/seasontype/3')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4y-R_mJS9JwL"
   },
   "outputs": [],
   "source": [
    "df_regular_season_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8Do2oRl9JwL"
   },
   "outputs": [],
   "source": [
    "df_postseason_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaxX1erg9JwL"
   },
   "outputs": [],
   "source": [
    "Off_Eff = []\n",
    "Def_Eff = []\n",
    "\n",
    "y = []\n",
    "\n",
    "iter = 0\n",
    "for year in range(2009, 2020):\n",
    "    url = 'http://www.espn.com/nba/hollinger/teamstats/_/year/' + str(year)\n",
    "    df_regular_season = pd.read_html(url)[0]\n",
    "    Off_Eff.append(df_regular_season.loc[2:, 10].values)\n",
    "    Def_Eff.append(df_regular_season.loc[2:, 11].values)\n",
    "    \n",
    "    N = len(df_regular_season.loc[2:, 1].values)\n",
    "    \n",
    "    y.append(-np.ones(N))\n",
    "    url += '/seasontype/3'\n",
    "    df_playoffs = pd.read_html(url)[0]\n",
    "    \n",
    "    playoff_teams = df_playoffs.loc[2:,1].values\n",
    "    all_teams = df_regular_season.loc[2:,1].values\n",
    "    for i in range(16):\n",
    "        for j in range(N):\n",
    "            if playoff_teams[i] == all_teams[j]:\n",
    "                y[iter][j] = 1\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6w07ufFd9JwM"
   },
   "outputs": [],
   "source": [
    "Off_Eff_All = np.concatenate(Off_Eff).astype('float')\n",
    "Def_Eff_All = np.concatenate(Def_Eff).astype('float')\n",
    "\n",
    "bball_features = np.vstack((Off_Eff_All.flatten(), Def_Eff_All.flatten())).T\n",
    "\n",
    "playoffs_All = np.concatenate(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sR9FOWfK9JwM"
   },
   "outputs": [],
   "source": [
    "idx_playoffs = np.where(playoffs_All==1)[0]\n",
    "idx_no_playoffs = np.where(playoffs_All==-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLdcC1Hi9JwM"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.scatter(bball_features[idx_playoffs, 0], bball_features[idx_playoffs, 1], \n",
    "            s=20, c='r', marker='s')\n",
    "\n",
    "plt.scatter(bball_features[idx_no_playoffs,0], bball_features[idx_no_playoffs, 1], \n",
    "            s=20, c='b', marker='o')\n",
    "\n",
    "plt.xlabel('OFF EFF', fontsize=14)\n",
    "plt.ylabel('DEF EFF', fontsize=14)\n",
    "\n",
    "plt.title('Playoff Teams (red) and Non-Playoff Teams (blue)', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOfwhlJV9JwM"
   },
   "source": [
    "### Okay, now what?\n",
    "\n",
    "The dataset above sure *looks* like it is reasonably separated into two distinct classes although there is a *bit* of overlap. \n",
    "\n",
    "In other words, this dataset is ***not*** linearly separable, which will mean bad things for the convergence of the training algorithm for the perceptron. We can still *try* to train a perceptron on this data. It just may not work. (Okay, it is *guaranteed* to not work.)\n",
    "\n",
    "However, we still need to address one key point: how do we *train* the perceptron?! In other words, how do we learn the weights and bias required to define the line that separates the *feature space* of offensive and defensive efficiency into playoff vs non-playoff teams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pqcf5spH9JwM"
   },
   "source": [
    "### Let's try to train a perceptron on the basketball data that are *almost* linearly separable (meaning, not linearly separable)\n",
    "\n",
    "- Well, convergence is *not* going to happen as there will always be miss-classification (which is just not acceptable for the perceptron).\n",
    "\n",
    "- Below, we explore how the \"line\" we learn jumps around as we use more and more iterations in the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6F_fz64zOG78"
   },
   "outputs": [],
   "source": [
    "# Step 1: Import\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# The \"dirty secret\" of needing to scale inputs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# We can use a pipeline to setup an order of operations of objects\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hV3MiohsBu54"
   },
   "outputs": [],
   "source": [
    "# Step 2: Instantiate\n",
    "stupid_neuron = make_pipeline(StandardScaler(), \n",
    "                              Perceptron(max_iter=100, tol=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KMLcTD9CQVH"
   },
   "outputs": [],
   "source": [
    "# Step 3: Fit\n",
    "x = bball_features\n",
    "y = playoffs_All\n",
    "\n",
    "stupid_neuron.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxbdHLMwCUIJ"
   },
   "outputs": [],
   "source": [
    "# Visualize performance\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    " \n",
    "x1_plot = np.linspace(np.min(x[:,0])-0.1*(np.max(x[:,0])-np.min(x[:,0])), \n",
    "                        np.max(x[:,0])+0.1*(np.max(x[:,0])-np.min(x[:,0])), \n",
    "                        51)\n",
    "x2_plot = np.linspace(np.min(x[:,1])-0.1*(np.max(x[:,1])-np.min(x[:,1])), \n",
    "                        np.max(x[:,1])+0.1*(np.max(x[:,1])-np.min(x[:,1])), \n",
    "                        51)\n",
    "x1_plot, x2_plot = np.meshgrid(x1_plot, x2_plot)\n",
    "\n",
    "x_plot = np.vstack((x1_plot.flatten(), x2_plot.flatten())).T\n",
    "\n",
    "y_plot = stupid_neuron.predict(x_plot)\n",
    "y_plot = y_plot.reshape(x1_plot.shape)\n",
    "plt.plot(figsize=(12,5))\n",
    "\n",
    "colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "plt.contourf(x1_plot, x2_plot, y_plot, alpha=0.2, cmap=cmap)\n",
    "\n",
    "idx_1 = np.where(y==1)[0]\n",
    "idx_0 = np.where(y==-1)[0]\n",
    "\n",
    "plt.scatter(x[idx_1,0], x[idx_1,1], s=50, c='r', marker='x')\n",
    "plt.scatter(x[idx_0,0], x[idx_0,1], s=50, c='b', marker='o')\n",
    "plt.title('Perceptron', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEODNlhiD8ZA"
   },
   "outputs": [],
   "source": [
    "# Maybe the optimization procedure for fitting needing more iterations?\n",
    "# Let's loop through different max iteration numbers\n",
    "\n",
    "max_iters = [500, 1000, 100000]\n",
    "\n",
    "neuron_list = []\n",
    "\n",
    "for max_iter in max_iters:\n",
    "    neuron_list.append(make_pipeline(StandardScaler(), \n",
    "                                     Perceptron(max_iter=max_iter, tol=None)))\n",
    "\n",
    "for (neuron, max_iter) in zip(neuron_list, max_iters):\n",
    "    neuron.fit(x, y)\n",
    "    y_plot = neuron.predict(x_plot)\n",
    "    y_plot = y_plot.reshape(x1_plot.shape)\n",
    "    plt.figure()\n",
    "    plt.plot(figsize=(12,5))\n",
    "\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    plt.contourf(x1_plot, x2_plot, y_plot , alpha=0.2, cmap=cmap)\n",
    "\n",
    "    idx_1 = np.where(y==1)[0]\n",
    "    idx_0 = np.where(y==-1)[0]\n",
    "\n",
    "    plt.scatter(x[idx_1,0], x[idx_1,1], s=50, c='r', marker='x')\n",
    "    plt.scatter(x[idx_0,0], x[idx_0,1], s=50, c='b', marker='o')\n",
    "    plt.title('Perceptron with max_iter={}'.format(max_iter), fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwyrR-wz9JwM"
   },
   "source": [
    "### Some closing remarks on the perceptron and foreshadowing\n",
    "---\n",
    "\n",
    "Linear separability is a ***strict requirement*** for the perceptron. \n",
    "\n",
    "The algorithms for learning the weights and bias in a perceptron will not work unless data are linearly separable. \n",
    "\n",
    "This is more than just mildly annoying, it is essentially impractical in many cases where we observe an ***almost*** linear separability.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/CU-Denver-MathStats-OER/Programming-for-Data-Science/blob/main/Lectures-and-Assignments/06-Machine-Learning/lectures/almost_linearly_separable_example.png?raw=1\" title='Almost separable' width=40%>\n",
    "\n",
    "<br>\n",
    "\n",
    "There is something called a ***kernel trick*** that is very common in machine learning. \n",
    "\n",
    "The basic premise is this: Maybe there is a function (called the kernel) that can transform the data in such a way that this transformed data are linearly separable. Then, we simply carry out our usual analysis except on this transformed data. \n",
    "\n",
    "When can we do this? Well, if it looks like there is a way to separate the data in some nonlinear way, then there is hope that we can figure out some kernel that allows us to linearly separate the transformed data. \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"https://github.com/CU-Denver-MathStats-OER/Programming-for-Data-Science/blob/main/Lectures-and-Assignments/06-Machine-Learning/lectures/not_linearly_separable_example.png?raw=1\" title='Nonlinearly separable illustration (what is the kernel trick?)' width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bi9bfIA89JwM"
   },
   "source": [
    "## [ADALINE = ADAptive LInear NEuron](https://en.wikipedia.org/wiki/ADALINE) <a name='adaline'>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnGvoPrZ9JwN"
   },
   "source": [
    "<mark> ***Key points:*** <mark>\n",
    "\n",
    "\n",
    "- When data are only *almost* linearly separable instead of absolutely linearly separable, we are going to have to live with some misclassifications (i.e., *predictions* given by $f(\\mathbf{w}^\\text{opt}\\cdot\\mathbf{x}^{(i)}+b^\\text{opt})$  that do not equal the reality of $y^{(i)}$ for some subset of $1\\leq i\\leq N$). \n",
    "\n",
    "- This means that what is *optimal* will in general mean that we minimize the misclassification percent rather than simply make it zero. This idea is explored more in our next neuron model ADALINE, so just be patient for now.\n",
    "\n",
    "- ADALINE is an alternative neuron model developed a few years after the perceptron was introduced. \n",
    "\n",
    "- Unlike the perceptron that learns the weights and bias based on the misclassification error being -2, 0, or 2 due to the use of a step-activation function acting on the *net input function* defined by $\\mathbf{w}\\cdot\\mathbf{x} + b$, the activation function in ADALINE is the identity function, which means we simply use the errors defined by $y^{(i)} - \\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)} + b\\right)$ to learn the weights and bias. \n",
    "\n",
    "- Once the weights and bias are learned, we still compute $f\\left(\\mathbf{w}\\cdot\\mathbf{x}+b\\right)$ to classify any particular feature vector $\\mathbf{x}$. The function $f$ from before is now considered a ***quantizer*** (as opposed to the activation function, which is now the identity function) that maps the net value into the class. \n",
    "\n",
    "We summarize this in the figure below. \n",
    "\n",
    "<img src=\"https://github.com/CU-Denver-MathStats-OER/Programming-for-Data-Science/blob/main/Lectures-and-Assignments/06-Machine-Learning/lectures/perceptron_vs_ADALINE.png?raw=1\" width=50%>\n",
    "\n",
    "<mark> ***Summary of main difference:*** </mark>\n",
    "\n",
    "> While $y$ is just -1's and 1's, the output of $\\left(\\mathbf{w}\\cdot\\mathbf{x} + b\\right)$ is *continuous*, so the *error* defined by $y^{(i)} - \\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)} + b\\right)$ does not just take on discrete values but instead provides a number corresponding to *how much* we are right/wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pXEikS09JwN"
   },
   "source": [
    "### How does this change the way in which we learn the weights?\n",
    "\n",
    "We first write the **sum of square errors (SSE)** from the $N$ training data as the *cost function* (sometimes called an *objective* function)\n",
    "$$\n",
    " \\large   J(\\mathbf{w},b) = \\frac{1}{2}\\sum_{i=1}^N \\left[y^{(i)}-\\left(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b\\right)\\right]^2. \n",
    "$$\n",
    "\n",
    "Observe that $J(\\mathbf{w},b)$ is a *quadratic* function over the space of weights and bias. The *objective* is to learn the weights and bias that *minimize* the *cost* function. This is conceptualized in the picture below.\n",
    "\n",
    "<img src=\"https://github.com/CU-Denver-MathStats-OER/Programming-for-Data-Science/blob/main/Lectures-and-Assignments/06-Machine-Learning/lectures/cost-functional.png?raw=1\" title=\"The cost function is like a big bowl\" width=50%>\n",
    "\n",
    "### There is a unique minimum?\n",
    "\n",
    "***Correct!*** Since the cost function is a quadratic function, it has a unique global minimum even if the data are not linearly separable! (This is the part where you say \"ooohhh\".)\n",
    "\n",
    "This means that you can always train an ADALINE...at least theoretically..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ji7MxKB59JwN"
   },
   "source": [
    "### Approximating the minimum with [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "\n",
    "First, a quick review of some useful facts from calculus and how we use them (you only need to understand how to apply these, so if you are not familiar with calculus, don't worry). \n",
    "\n",
    "- The [gradient](https://en.wikipedia.org/wiki/Gradient) of a scalar valued multivariate function evaluated at some input is a *vector* of derivatives (partial derivatives to be exact) that points in the direction of greatest increase of function values. This means that the *negative* of the gradient at some input will point in the direction of greatest *decrease* of function values.  \n",
    "\n",
    "   - Why is this important to us? We want to minimize a function, so we will be using the *negative gradient*.\n",
    "\n",
    "   - What do we need to be aware of? The *length* of the gradient vector (also called its magnitude) can complicate the decision making process in terms of how far we should step in the direction of the negative gradient.\n",
    "\n",
    "       - If the length of the gradient vector is too large, then we may step *too far* so that we end up *increasing* the value of the function instead of *decreasing* the value! This is just plain counterproductive. \n",
    "\n",
    "       - If the length of the gradient is too small or we just choose to make too small of steps, then it may take *way too long* to have this method converge. \n",
    "\n",
    "The image below may prove useful to refer as it summarizes why we move in the direction of the negative gradient.\n",
    "\n",
    "<img src=\"https://github.com/CU-Denver-MathStats-OER/Programming-for-Data-Science/blob/main/Lectures-and-Assignments/06-Machine-Learning/lectures/cost-functional-gradient.png?raw=1\" title=\"The negative direction of gradient is what we want!\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90sxhpHeEtdy"
   },
   "outputs": [],
   "source": [
    "# Step 1: Import the \"stochastic gradient descent\" (SGD) classifiers\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier as SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_UbwugrEOct"
   },
   "outputs": [],
   "source": [
    "# Step 2: Instantiate\n",
    "less_stupid_neuron = make_pipeline(StandardScaler(), \n",
    "                                   SGD(loss = 'squared_error', \n",
    "                                       penalty=None, tol=None, max_iter=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NN-s18SlEiTL"
   },
   "outputs": [],
   "source": [
    "# Step 3: Fit\n",
    "less_stupid_neuron.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqcgCqUgEpr2"
   },
   "outputs": [],
   "source": [
    "y_plot = less_stupid_neuron.predict(x_plot)\n",
    "y_plot = y_plot.reshape(x1_plot.shape)\n",
    "plt.plot(figsize=(12,5))\n",
    "\n",
    "colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "plt.contourf(x1_plot, x2_plot, y_plot, alpha=0.2, cmap=cmap)\n",
    "\n",
    "idx_1 = np.where(y==1)[0]\n",
    "idx_0 = np.where(y==-1)[0]\n",
    "\n",
    "plt.scatter(x[idx_1,0], x[idx_1,1], s=50, c='r', marker='x')\n",
    "plt.scatter(x[idx_0,0], x[idx_0,1], s=50, c='b', marker='o')\n",
    "plt.title('ADALINE', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SR7NM9RGM_NS"
   },
   "outputs": [],
   "source": [
    "df_regular_season_2023 = pd.read_html('http://www.espn.com/nba/hollinger/teamstats/_/year/2023')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3UmIkYbFFJI4"
   },
   "outputs": [],
   "source": [
    "playoff_predictions = less_stupid_neuron.predict(df_regular_season_2023.loc[2:, [10, 11]].values.astype(float)).astype(str)\n",
    "playoff_predictions[playoff_predictions=='-1.0'] = 'No'\n",
    "playoff_predictions[playoff_predictions=='1.0'] = 'Yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqZKkuWcTTI8"
   },
   "outputs": [],
   "source": [
    "df_temp = df_regular_season_2023.loc[2:, [1, 10, 11]]\n",
    "df_temp.rename(columns={1: \"Team\", 10: \"Off. Eff.\", 11: \"Def. Eff.\"}, inplace=True)\n",
    "df_temp.reset_index(drop=True, inplace=True)\n",
    "df_temp['Playoffs'] = playoff_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqMTjhY6M2eN"
   },
   "outputs": [],
   "source": [
    "def highlight_playoffs(s):  # Highlight the playoff teams\n",
    "    if s.Playoffs == 'Yes':\n",
    "        return ['background-color: yellow']*4\n",
    "    else:\n",
    "        return ['background-color: black']*4\n",
    "\n",
    "\n",
    "df_temp.style.apply(highlight_playoffs, axis=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PgbhZyz8xchB",
    "jiXRgewrxchB",
    "7kI6VLDJxchD"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9b7b09fe4b9fb09965c3662f2214756554a4e56d1370f403412e2d0f56132c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
